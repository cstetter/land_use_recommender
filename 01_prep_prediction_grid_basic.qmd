---
title: "Build prediction dataframe"
format: html
---

# Intro
The given code performs several operations to build a prediction dataframe based on weather history at the grid level. Let's break down the code and explain each block:

The code begins by loading the required libraries for data manipulation, spatial operations, statistical modeling, and others.
```{r}
library(tidyverse)
library(raster)
library(rgeos)
library(sf)
library(terra)
library(gstat)
library(stars)
library(rdwd)
library(mlogit)
library(abind)
library(DescTools)
library(pbapply)
library(parallel)
```


The next section focuses on weather history at the grid level. It starts by loading monthly gridded data from the DWD (Deutscher Wetterdienst) database.
```{r}
# Load string vector with all vailable directories (CDC)
data("gridIndex")
```

The code filters the relevant years and the growing season subset. It selects directories with monthly gridded data from March to September for the 10 years prior to 2020.
```{r}
years=as.character(2010:2019)

base_dwd <- as.data.frame(gridIndex) %>%
 mutate("month"=as.numeric(str_sub(gridIndex,-09,-8)),
        "year"=as.numeric(str_sub(gridIndex,-13,-10))) %>% 
 filter(year %in% years, 
        month %in% 3:10)


# growing season precipitation string index
precip_index <- base_dwd  %>% 
 filter(str_detect(gridIndex, "monthly_precipitation"))

# growing season mean temperature string index
temp_index <- base_dwd  %>% 
 filter(str_detect(gridIndex, "monthly_air_temp_mean"))
```

Monthly data is downloaded using the created string indices.
```{r}
raster_rain <- stack(dataDWD(precip_index[[1]], base=gridbase, joinbf=TRUE, dir=locdir()))
raster_temp <- stack(dataDWD(temp_index[[1]], base=gridbase, joinbf=TRUE, dir=locdir()))
```

The code assigns a projection (EPSG:31467) to the downloaded raster datasets (https://opendata.dwd.de/climate_environment/CDC/grids_germany/monthly/air_temperature_mean/DESCRIPTION_gridsgermany_monthly_air_temperature_mean_en.pdf).
```{r}
crs(raster_rain) <- crs(raster_temp) <- "EPSG:31467"
```


The Bavarian polygon is added to mask the data and keep only the region of Bavaria.
```{r}
shapeBY <- st_read("../dat/spatial_data/bayern_ex.shp")[,"SCH"]

## Match projections
shapeBY <- st_transform(shapeBY, st_crs(raster_rain))
```

Unnecessary files are removed to save memory.
```{r}
rm(gridIndex, precip_index, temp_index)
```

The raster datasets are cropped and masked to include only the region of Bavaria.
```{r}
raster_temp <- crop(raster_temp, shapeBY)
raster_rain <- crop(raster_rain, shapeBY)

raster_temp <- mask(raster_temp, shapeBY)
raster_rain <- mask(raster_rain, shapeBY)
```


# Yearly average temperature growing season

First, two sets of years are defined: years_1_3 for the years 2017 to 2019 and years_4_10 for the years 2010 to 2016
```{r}
years_1_3 = as.character(2017:2019)
years_4_10 = as.character(2010:2016)
```


The code calculates the mean temperature for the years 2017 to 2019 (years_1_3) and the years 2010 to 2016 (years_4_10) from the raster_temp dataset.
```{r}
temp_1_3 <- mean(
 raster_temp[[tidyselect::vars_select(
  names(raster_temp), 
  contains(years_1_3))]],
 na.rm=T
)


temp_4_10 <- mean(
 raster_temp[[tidyselect::vars_select(
  names(raster_temp), 
  contains(years_4_10))]],
 na.rm=T
)
```

# Yearly rainfall sum growing season
Yearly rainfall sum during the growing season is calculated. The results are divided by the number of years in each period (years_1_3 and years_4_10) and multiplied by 10 (to retrieve percipitation sum in mm).
```{r}
rain_1_3 <- sum(
 raster_rain[[tidyselect::vars_select(
        names(raster_rain), 
        contains(years_1_3))]],
 na.rm=T
) / length(years_1_3) * 10


rain_4_10 <- sum(
 raster_rain[[tidyselect::vars_select(
        names(raster_rain), 
        contains(years_4_10))]],
 na.rm=T
) / length(years_4_10) * 10
```



# Indicators based on daily data
Other indicators are calculated using daily data. First, the growing_season vector is created by combining the years and months in a specific format.
```{r}
growing_season <- paste0(rep(years, each=8), ".", c(rep(0,7), 1) , c(3:9, 0), ".")
```


Daily rainfall and maximum temperature data files are loaded and cropped to the Bavaria shape. The shape projection is matched with the data.
```{r}
files_rain_daily <- list.files('../dat/dwd_data/rain',
                               pattern='*.nc', 
                               full.names=TRUE)
rain_d <- lapply(files_rain_daily, stack)

files_Tmax_daily <- list.files('../dat/dwd_data/tmax',
                               pattern='*.nc', 
                               full.names=TRUE)
Tmax_d <- lapply(files_Tmax_daily, stack)

shapeBY <- st_transform(shapeBY, st_crs(rain_d[[1]]))

rain_d_gs <-  pblapply(rain_d, function(i) i
                     [[tidyselect::vars_select(
                      names(i),
                      contains(growing_season))]])


Tmax_d_gs <-  pblapply(Tmax_d, function(i) i
                     [[tidyselect::vars_select(
                      names(i),
                      contains(growing_season))]])

raster_rain_d_gs  <- pblapply(rain_d_gs, function(i)
 crop(i, shapeBY))

raster_Tmax_d_gs  <- pblapply(Tmax_d_gs, function(i)
 crop(i, shapeBY))

raster_rain_d_gs <- stack(raster_rain_d_gs)
raster_Tmax_d_gs <- stack(raster_Tmax_d_gs)
```

## R20
The r20 function is defined to calculate the number of days with rainfall above a threshold (t). The function is applied to the cropped rainfall data for the specified years, and the result is divided by the number of years.
```{r}
r20 <- function(x,t){
 z <- sum(x > t)
 return(z)
}


r20_1_3 <- r20(
 raster_rain_d_gs[[tidyselect::vars_select(
  names(raster_rain_d_gs), 
  contains(years_1_3))]], t = 20) / length(years_1_3) 

r20_4_10 <- r20(
 raster_rain_d_gs[[tidyselect::vars_select(
  names(raster_rain_d_gs), 
  contains(years_4_10))]], t = 20) / length(years_4_10)
```

## Dry days
Similarly, the dd function calculates the number of dry days (rainfall below a threshold t). The function is applied to the cropped rainfall data for the specified years, and the result is divided by the number of years.

```{r}
dd <- function(x,t){
 z <- sum(x < t)
 return(z)
}

dd_1_3 <- dd(
 raster_rain_d_gs[[tidyselect::vars_select(
  names(raster_rain_d_gs), 
  contains(years_1_3))]], t = 1) / length(years_1_3) 

dd_4_10 <- dd(
 raster_rain_d_gs[[tidyselect::vars_select(
  names(raster_rain_d_gs), 
  contains(years_4_10))]], t = 1) / length(years_4_10)
```

## Hot days
The hd function calculates the number of hot days (maximum temperature above a threshold t). The function is applied to the cropped maximum temperature data for the specified years, and the result is divided by the number of years.
```{r}
hd <- function(x,t){
 z <- sum(x > t)
 return(z)
}

hd_1_3 <- hd(
 raster_Tmax_d_gs[[tidyselect::vars_select(
  names(raster_Tmax_d_gs), 
  contains(years_1_3))]], t = 29) / length(years_1_3) 

hd_4_10 <- hd(
 raster_Tmax_d_gs[[tidyselect::vars_select(
  names(raster_Tmax_d_gs), 
  contains(years_4_10))]], t = 29) / length(years_4_10)
```


# Combine data and fit to 1km EEA raster
This section loads a spatial grid of 1km resolution for Bavaria (provided by the European Environment Agency) and creates spatial data frames (df_hd, df_dd, df_r20, df_temp, df_rain) by converting raster stacks into sf objects and transforming them to match the coordinate system of the grid.

```{r}
gridBY <- st_read("../dat/grid_de_1km/bavaria_grid_1km.gpkg",
                  layer = "bavaria_grid_1km")

df_hd <- 
 st_as_sf(as(stack(hd_1_3, hd_4_10), 'SpatialPolygonsDataFrame')) %>% 
 rename(hd_1_3=layer.1, hd_4_10=layer.2) %>% 
 st_transform(st_crs(gridBY))
 

df_dd <- 
 st_as_sf(as(stack(dd_1_3, dd_4_10), 'SpatialPolygonsDataFrame')) %>% 
 rename(dd_1_3=layer.1, dd_4_10=layer.2) %>% 
 st_transform(st_crs(gridBY))

df_r20 <- 
 st_as_sf(as(stack(r20_1_3, r20_4_10), 'SpatialPolygonsDataFrame')) %>% 
 rename(r20_1_3=layer.1, r20_4_10=layer.2) %>% 
 st_transform(st_crs(gridBY))

df_temp <- 
 st_as_sf(as(stack(temp_1_3, temp_4_10), 'SpatialPolygonsDataFrame')) %>% 
 rename(temp_1_3=layer.1, temp_4_10=layer.2) %>% 
 st_transform(st_crs(gridBY))

df_rain <- 
 st_as_sf(as(stack(rain_1_3, rain_4_10), 'SpatialPolygonsDataFrame')) %>% 
 rename(rain_1_3=layer.1, rain_4_10=layer.2) %>% 
 st_transform(st_crs(gridBY))
```

Plotting the data: This section creates a plot using ggplot2 library to visualize the spatial data frames overlaid on the grid. The different data layers are represented by different colors.
```{r}
ggplot() +
 geom_sf(data = st_crop(gridBY, st_bbox(df_temp[1:16,])), aes(), col="black", fill=NA)+
 geom_sf(data = st_crop(df_hd, st_bbox(df_temp[1:16,])),aes(), col="red", fill=NA) +
 geom_sf(data = st_crop(df_dd, st_bbox(df_temp[1:16,])),aes(), col="blue", fill=NA) +
 geom_sf(data = st_crop(df_r20, st_bbox(df_temp[1:16,])),aes(), col="yellow", fill=NA) +
 geom_sf(data = st_crop(df_rain, st_bbox(df_temp[1:16,])),aes(), col="orange", fill=NA) +
 geom_sf(data = df_temp[1:16,],aes(), col="green", fill=NA) +
  theme_bw()
```


Based on the above plot, merge data with the same borders: This section combines the spatial data frames with the same borders to speed up the process. The st_join function is used with appropriate spatial join operations (st_equals and st_intersects) to merge the data frames.
```{r}
df_temprain <- st_join(df_temp, df_rain, join=st_equals)
df_ddr20 <- st_join(df_r20, df_dd, join=st_equals)
df3 <- st_join(df_ddr20, df_hd, join = st_intersects, largest=T)
```

Fit data to the EAA raster: In this section, the data frames are fitted to the EAA raster (gridBY) using parallel computing with the help of the pblapply function. The clusters are created, and the necessary libraries and data are exported. Then, the st_interpolate_aw function is applied to each data frame, and the results are reduced using purrr::reduce. Finally, the clusters are stopped.
```{r}
cl <- makeCluster(6)

clusterEvalQ(cl, {
 library(sf)
 library(pbapply)
})

clusterExport(cl, c("df_temprain", "df3", "gridBY"))

df_a <- pblapply(1:4, function(i) 
 st_interpolate_aw(df_temprain[,i], gridBY, extensive=F), cl=cl) %>% 
 purrr::reduce(st_join, join=st_equals)

df_b <- pblapply(1:6, function(i) 
 st_interpolate_aw(df3[,i], gridBY, extensive=F), cl=cl) %>% 
 purrr::reduce(st_join, join=st_equals)

stopCluster(cl)
```

Create the dataset: This section combines the interpolated data frames (df_a and df_b) using st_join with spatial join operation st_equals and removes any rows with missing values using na.omit.
```{r}
df_dwd <- st_join(df_a, df_b, join=st_equals) %>% na.omit()

saveRDS(df_dwd, "../dat/data_processed/df_dwd1km.rds")
```

Check data graphically: This section creates another plot using ggplot2 to visualize the spatial data frames df_prediction and df_temprain overlaid on the grid. The fill aesthetic is used to represent the rain_1_3 variable.
```{r}
ggplot() +
 geom_sf(data = st_crop(df_dwd, st_bbox(df_temp[1:16,])),
         aes(fill=rain_1_3), col="black") +
 geom_sf(data = st_crop(df_temprain, st_bbox(df_temp[1:16,])), aes(),
         col="red", fill=NA) +
 theme_bw()
```


```{r}
# Read the 'df_dwd' data frame from an RDS file
df_dwd <- readRDS("../dat/data_processed/df_dwd1km.rds")

# Convert selected columns of 'df_dwd' to 5km resolution rasters, calculating the mean
dwd_ras <- lapply(colnames(df_dwd)[1:10], function(i) {
  rast(raster(df_dwd, res = 5000)) %>% 
    rasterize(df_dwd, ., field = df_dwd[[i]], fun = "mean")
})

# Convert the first raster in 'dwd_ras' to an 'sf' object and extract the geometry
geometry <- st_as_sf(st_as_stars(dwd_ras[[1]]))$geometry

# Create a data frame by combining all the rasters and dropping their geometries
df_dwd_5km <- do.call(cbind, lapply(dwd_ras, function(i) {
  st_drop_geometry(st_as_sf(st_as_stars(i)))
}))

# Add the extracted geometry back to the data frame
df_dwd_5km <- st_as_sf(cbind(df_dwd_5km, geometry))

# Rename the columns of 'df_dwd_5km' to match 'df_dwd'
colnames(df_dwd_5km) <- colnames(df_dwd)

saveRDS(df_dwd_5km, "../dat/data_processed/df_dwd5km.rds")
```


# Mean-center weather history data
The originally used data in Stetter, Sauer (2022) was mean-centered. 
We use same means as in the original paper to make datasets compatible.

This section involves loading the original processed weather data based on zip codes from the file fd_weather_plz.rds and performing mean-centering. The code calculates the means of specific weather variables (rain, temp, dd, r20, hd) for two time periods (1-3 lags and 4-10 lags) and creates a new data frame center_weather containing the means.
```{r}
# Load original processed weather data based on zip codes
weather_stetter_sauer <-  readRDS("../dat/choice_data/fd_weather_plz.rds") 

colnames(weather_stetter_sauer) <- str_replace(
 colnames(weather_stetter_sauer), "ann_", "") 

old_names <- c("rain", "temp", "dd", "r20", "hd")

center_weather <- cbind(
 ## 1-3 lags
 weather_stetter_sauer %>% filter(year>=2017 & year<=2019) %>%
  aggregate(., by = list(.$plz), FUN = mean) %>% 
  dplyr::select(Group.1, rain:hd) %>%
  rename_at(vars(colnames(.)), ~ c("zip", paste0(old_names, "_1_3" ))),
 
 ## 4-10 lag
 weather_stetter_sauer %>% filter(year>=2010 & year<=2016) %>%
  aggregate(., by = list(.$plz), FUN = mean) %>% 
  dplyr::select(rain:hd) %>%
  rename_at(vars(colnames(.)), ~ c(paste0(old_names, "_4_10" )))
) %>% 
 dplyr::select(-zip) %>% 
 colMeans()

# re-order variables to align with created prediction dwd weather dataset
center_weather <- center_weather[c("temp_1_3",  "temp_4_10",
                                   "rain_1_3", "rain_4_10",
                                   "r20_1_3", "r20_4_10",
                                   "dd_1_3", "dd_4_10",
                                   "hd_1_3", "hd_4_10")] 

df_dwd_centered <- scale(st_drop_geometry(df_dwd), 
                         center_weather, 
                         scale = FALSE)

df_dwd_centered <- as.data.frame(df_dwd_centered)
colnames(df_dwd_centered) <- paste0(colnames(df_dwd_centered), "_c")
df_dwd_centered$geom <- df_dwd$geometry
df_dwd_centered <- st_as_sf(df_dwd_centered)
```


# Land cover
```{r}
#### Land cover ####
# Load land cover 10m grid
files_terra <- list.files('../dat/terrascope_10m',
                          pattern='*.tif', 
                          full.names=TRUE)

# Load
ic <- lapply(files_terra, rast)

# Align prjections
shape <- st_transform(gridBY, crs(ic[[1]]))

# Build single raster of the 6 raster files within Bavaria
ic <- sprc(lapply(ic, function(i) crop(i, shape)))
r <- mosaic(ic)
r <- raster(r)
```


```{r}
# Calculate land type shares
calc_share <- function(cells){ 
 x <- raster::crop(r, shape[cells,])
 df_res <- as.data.frame(zonal(area(x), x, sum))
 df_res$share <- with(df_res, value/sum(value))

 res <- df_res %>% select(1,3)  %>% 
  pivot_wider(names_from = zone, values_from = share, names_prefix = "share")
 return(res)
}



cl <- makeCluster(7)
clusterEvalQ(cl, {
 library(sf)
 library(pbapply)
 library(raster)
 library(tidyverse)
})

clusterExport(cl, c("r", "shape", "calc_share"))

lu_shares <- pblapply(1:nrow(shape), calc_share, cl=cl)

stopCluster(cl)
```


```{r}
lu_codes= c(tree_cover="share10",
        shrubland="share20",
        grassland="share30",
        cropland="share40",
        built_up="share50",
        bare_vegetation="share60",
        snow_ice="share70",
        water_body="share80",
        herbaceous_wetland="share90",
        moss="share100")

df_lu_shares <- data.table::rbindlist(lu_shares, fill=TRUE) %>% 
 replace(is.na(.), 0) %>% 
 rename(all_of(lu_codes)) %>% 
 as.data.frame()


df_lu_shares <- cbind(df_lu_shares, gridBY)
```

# Final Prediction data
```{r}
df_pred_geom <- st_join(df_dwd_centered, 
                        st_as_sf(df_lu_shares), 
                        join=st_equals) %>% 
 filter(cropland!=0)

df_pred <- st_drop_geometry(df_pred_geom)
```

# Save data
```{r}
write.csv(df_pred, file = "../dat/data_processed/df_prediction.csv", 
          row.names = F)
st_write(df_pred_geom, "../dat/data_processed/df_prediction.gpkg",append = F)
```