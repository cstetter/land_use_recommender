---
title: "Untitled"
format: 
 html:
    toc: true
    html-math-method: katex
    css: styles.css
editor: visual
---

# 1 Load packages and data

## 1.1. Packages

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(stringr)
library(dplyr)
library(rgdal)
library(raster) 
library(rgeos)
library(sf)
library(mlogit)

library(fGarch) # skewed normal dist
library(abind) # combine arrays
# Statistical mode
library(DescTools) 

library(reticulate) # R Python interface
```

## 1.2. weather data

## ECAD 
```{r}
raster_temp <- stack("../dat/weather_data/tg_ens_mean_0.1deg_reg_v24.0e.nc")
raster_rain <- stack("../dat/weather_data/rr_ens_mean_0.1deg_reg_v24.0e.nc")
raster_Tmax <- stack("../dat/weather_data/tx_ens_mean_0.1deg_reg_v23.0e.nc")
```


## 1.3. Shape data BY

```{r}
## add choice experiment data to allocate observations in space
dat <- readRDS("../dat/choice_data/data_calc.rds")
shapeBY <- st_read("../dat/spatial_data/bayern_ex.shp")[,"SCH"]

## Match projections
shapeBY <- st_transform(shapeBY, st_crs(raster_temp))
```

## 1.4. Focus on growing season Mar-Oct

```{r}
growing_season <- paste0(rep(seq(2000,2019), each=8), ".", c(rep(0,7), 1) , c(3:9, 0), ".")
years <- seq(2000,2019) %>% as.character()

raster_rain_gs <- raster_rain[[tidyselect::vars_select(
  names(raster_rain),
  contains(growing_season))]]

raster_temp_gs <- raster_temp[[tidyselect::vars_select(
  names(raster_temp),
  contains(growing_season))]]

raster_Tmax_gs <- raster_Tmax[[tidyselect::vars_select(
  names(raster_Tmax),
  contains(growing_season))]]
```

## 1.5 Shrink raster dataset

```{r}
raster_temp <- crop(raster_temp_gs, shapeBY)
raster_rain <- crop(raster_rain_gs, shapeBY)
raster_Tmax <- crop(raster_Tmax_gs, shapeBY)
```


## 2.1 Yearly average temperature growing season

### Annual GS data at raster level

```{r}
list_temp <- lapply(years, function(i) 
  raster_temp[[tidyselect::vars_select(
    names(raster_temp), 
    contains(i))]])

names(list_temp) <- years

list_temp_ann <- lapply(years %>% as.character, function(x) 
 stackApply(list_temp[[x]], indices = 1, fun='mean'))
names(list_temp_ann) <- years

#--- convert to an sf object ---#
list_temp <- lapply(list_temp_ann, function(i)
 st_as_sf(
  as(i, 'SpatialPolygonsDataFrame')
 )
)

# clip rasters
# a <- sf_data[lengths(st_intersects(sf_data, shapeBY)) ==1,]
b <- lapply(list_temp, function(i) st_intersection(i, shapeBY)) 


# Make dataframe
df_temp <- b %>% 
 purrr::map(dplyr::select, 1) %>% 
 purrr::reduce(st_join, join=st_equals)
colnames(df_temp) <- c(paste0("X", 2000:2019), "geometry") 

 ## weather history in 2020
temp_1_3 <- rowMeans(
  select(st_drop_geometry(df_temp), X2017:X2019))

temp_4_10 <- rowMeans(
  select(st_drop_geometry(df_temp), X2010:X2016))
 
# ggplot(data=df_facet) +
#     geom_sf(data = shapeBY, color = gray(.5), alpha=0.2) +
# geom_sf( aes(fill = value)) +
#  facet_grid(~name) +
#  theme_void
```

## 2.2 Yearly precip sum - growing season

### Annual GS data at raster level

```{r}
list_rain <- lapply( years, function(i) 
  raster_rain[[tidyselect::vars_select(
    names(raster_rain), 
    contains(i))]])
names(list_rain) <- years

list_rain_ann <- lapply(years %>% as.character, function(x) 
 stackApply(list_rain[[x]], indices = 1, fun='sum'))
names(list_rain_ann) <- years


#--- convert to an sf object ---#
list_rain1 <- lapply(list_rain_ann, function(i)
 st_as_sf(
  as(i, 'SpatialPolygonsDataFrame')
 )
)

# clip rasters
# a <- sf_data[lengths(st_intersects(sf_data, shapeBY)) ==1,]
b <- lapply(list_rain1, function(i) st_intersection(i, shapeBY)) 


# Make dataframe
df_rain <- b %>% 
 purrr::map(dplyr::select, 1) %>% 
 purrr::reduce(st_join, join=st_equals)
colnames(df_rain) <- c(paste0("X", 2000:2019), "geometry") 

 ## weather history in 2020
rain_1_3 <- rowMeans(
  select(st_drop_geometry(df_rain), X2017:X2019))

rain_4_10 <- rowMeans(
  select(st_drop_geometry(df_rain), X2010:X2016))
```

## 2.3 Yearly maximum temperature growing season

### Yearly max temp data at raster level

```{r}
list_Tmax <- lapply( years, function(i) 
  raster_Tmax[[tidyselect::vars_select(
    names(raster_Tmax), 
    contains(i))]])
names(list_Tmax) <- years
```

### Hot days

```{r}
## formula count hot days
hd <- function(x,t){
  z <- sum(x > t)
  return(z)
}

## calculate hot days in ag season
list_hd <- lapply(years, function(x) 
 calc(list_Tmax[[x]], function(i) hd(i, 29)))
names(list_hd) <- years


#--- convert to an sf object ---#
list_hd <- lapply(list_hd, function(i)
 st_as_sf(
  as(i, 'SpatialPolygonsDataFrame')
 )
)

# clip rasters
# a <- sf_data[lengths(st_intersects(sf_data, shapeBY)) ==1,]
b <- lapply(list_hd, function(i) st_intersection(i, shapeBY)) 


# Make dataframe
df_hd <- b %>% 
 purrr::map(dplyr::select, 1) %>% 
 purrr::reduce(st_join, join=st_equals)
colnames(df_hd) <- c(paste0("X", 2000:2019), "geometry") 

 ## weather history in 2020
hd_1_3 <- rowMeans(
  select(st_drop_geometry(df_hd), X2017:X2019))

hd_4_10 <- rowMeans(
  select(st_drop_geometry(df_hd), X2010:X2016))
```

## 2.4 Yearly dry days growing season

### Gather data within plz boundaries

```{r}
## formula count dry days
dd <- function(x,t){
  z <- sum(x < t)
  return(z)
}

list_dd <- lapply(years, function(x) 
 calc(list_rain[[x]], function(i) dd(i, 1)))
names(list_dd) <- years


#--- convert to an sf object ---#
list_dd <- lapply(list_dd, function(i)
 st_as_sf(
  as(i, 'SpatialPolygonsDataFrame')
 )
)

# clip rasters
# a <- sf_data[lengths(st_intersects(sf_data, shapeBY)) ==1,]
b <- lapply(list_dd, function(i) st_intersection(i, shapeBY)) 


# Make dataframe
df_dd <- b %>% 
 purrr::map(dplyr::select, 1) %>% 
 purrr::reduce(st_join, join=st_equals)
colnames(df_dd) <- c(paste0("X", 2000:2019), "geometry") 

 ## weather history in 2020
dd_1_3 <- rowMeans(
  select(st_drop_geometry(df_dd), X2017:X2019))

dd_4_10 <- rowMeans(
  select(st_drop_geometry(df_dd), X2010:X2016))
```

## 2.5 Yearly heavy rain days growing season

### Gather data within plz boundaries

```{r}
## formula count hot days
r20 <- function(x,t){
  z <- sum(x > t)
  return(z)
}

list_r20 <- lapply(years, function(x) 
 calc(list_rain[[x]], function(i) r20(i, 20)))
names(list_r20) <- years


#--- convert to an sf object ---#
list_r20 <- lapply(list_r20, function(i)
 st_as_sf(
  as(i, 'SpatialPolygonsDataFrame')
 )
)

# clip rasters
# a <- sf_data[lengths(st_intersects(sf_data, shapeBY)) ==1,]
b <- lapply(list_r20, function(i) st_intersection(i, shapeBY)) 


# Make dataframe
df_r20 <- b %>% 
 purrr::map(dplyr::select, 1) %>% 
 purrr::reduce(st_join, join=st_equals)
colnames(df_r20) <- c(paste0("X", 2000:2019), "geometry") 

 ## weather history in 2020
r20_1_3 <- rowMeans(
  select(st_drop_geometry(df_r20), X2017:X2019))

r20_4_10 <- rowMeans(
  select(st_drop_geometry(df_r20), X2010:X2016))
```

# 4 Mean-center data

The originally used data in Stetter, Sauer (2022) was mean-center. We use same means as in the original paper to make datasets compatible.

```{r warning=FALSE}
# Load original processed weather data based on zip codes
weather_stetter_sauer <-  readRDS("../dat/choice_data/fd_weather_plz.rds") 

colnames(weather_stetter_sauer) <- str_replace(
 colnames(weather_stetter_sauer), "ann_", "") 

old_names <- c("rain", "temp", "dd", "r20", "hd")

center_weather <- cbind(
 ## 1-3 lags
 weather_stetter_sauer %>% filter(year>=2017 & year<=2019) %>%
  aggregate(., by = list(.$plz), FUN = mean) %>% 
  dplyr::select(Group.1, rain:hd) %>%
  rename_at(vars(colnames(.)), ~ c("zip", paste0(old_names, "_1_3" ))),
 
 ## 4-10 lag
 weather_stetter_sauer %>% filter(year>=2010 & year<=2016) %>%
  aggregate(., by = list(.$plz), FUN = mean) %>% 
  dplyr::select(rain:hd) %>%
  rename_at(vars(colnames(.)), ~ c(paste0(old_names, "_4_10" )))
) %>% 
 dplyr::select(-zip) %>% 
 colMeans()

# re-order variables to align with created raster weather dataset
center_weather <- center_weather[c("rain_1_3", "rain_4_10",
                                   "temp_1_3",  "temp_4_10",
                                   "dd_1_3", "dd_4_10",
                                   "r20_1_3", "r20_4_10",
                                   "hd_1_3", "hd_4_10")] 
```

```{r}
dfc <- as.data.frame(
  cbind(
    rain_1_3,
    rain_4_10,
    temp_1_3,
    temp_4_10,
    dd_1_3,
    dd_4_10,
    r20_1_3,
    r20_4_10,
    hd_1_3,
    hd_4_10))


dfc <- dfc %>% 
  mutate(geometry=df_dd$geometry) %>% 
  st_as_sf()

## mean-center weather variables 
for (i in 1:10) {
 dfc[[paste0(colnames(dfc)[i], "_c")]] <-
  dfc[[i]] - center_weather[i]
}


# Save data, which will be test data for MAB
st_write(dfc,"../dat/data_processed/weather_agents_recommender.gpkg", 
         delete_dsn = T)
```


# 5 Calculate expected utility

## 5.1 Load choice model from Stetter, Sauer (2023)

```{r}
# Load original processed weather data based on zip codes
res_stetter_sauer <-  readRDS("../dat/choice_data/res_cor_13_410.rds") 
```

## 5.2 Predicted utility when constant coefficients are assumed

```{r}
# Extract model matrix
mm <- model.matrix(res_stetter_sauer)

# Extract mean coefficents
coef_stetter_sauer <- coef(res_stetter_sauer)[1:27]

# Return margin follows log distr. -> exponent
coef_stetter_sauer["DB"] <- exp(coef_stetter_sauer["DB"])

coefsubset <- intersect(names(coef_stetter_sauer), colnames(mm))

mm <- mm[, coefsubset]
utiltiy_homo <- 
 crossprod(
  t(mm[, coefsubset]), 
  coef_stetter_sauer[coefsubset]
 )

```

## 5.3 Predicted utility when farm-level coefficients are assumed

```{r}
# Prepare indiv. coefficients for row-wise multiplication
ind_par <- as.matrix(t(purrr::map_dfr(seq_len(36), ~res_stetter_sauer$indpar)))

# Calculate linpred w/ heterogeneous coefficients
utility_hetero <- sapply(1:nrow(mm), function(i)
 mm[i,] %*%
  ind_par[-1,i]
)

utility_hetero_norm <- scales::rescale(utility_hetero, to=c(0,1))
```

## Standardized probability-weighted U

```{r}
utility_hetero_prob <- 
 as.vector(scales::rescale(utility_hetero, to=c(0,1))*
            res_stetter_sauer$model$probabilities)
```

# 6 Build training dataframe

```{r}
df_train <- res_stetter_sauer$model  %>%  
  as.data.frame %>% 
  dplyr::select(DB:hd_4_10_c) %>% 
  dplyr::mutate(reward_homo=utiltiy_homo,
                reward_hetero=utility_hetero,
                reward_hetero_prob=utility_hetero_prob,
                reward_hetero_norm=utility_hetero_norm,
                decision_arm=rep(c("SRC", "AC", "SQ"),nrow(mm)/3)) %>% 
 dplyr::filter(!is.na(reward_hetero_prob))
write.csv(df_train, file = "../dat/data_processed/df_train", row.names = F)
```

# 7 Build test dataframe

```{r}
df_test <- dfc %>% 
  dplyr::select(contains("_c")) %>% 
  st_drop_geometry() 



write.csv(df_test, file = "../dat/data_processed/df_test", row.names = F)


# write.csv(array_weather[,,500], file = "../dat/data_processed/df_test", row.names = F)
```

# 8 Train multi-armed bandit (MAB) model

Load packages

```{python}
import pandas as pd
from sklearn.preprocessing import StandardScaler
from mabwiser.mab import MAB, LearningPolicy, NeighborhoodPolicy
```

Load data

```{python}
df_train = pd.read_csv("../dat/data_processed/df_train")
df_test = pd.read_csv("../dat/data_processed/df_test")
# df_test = pd.read_csv("../dat/data_processed/df_test_30")

# Arms
actions = ['SRC', 'AC', 'SQ']
```

Scale data

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Scale the training and test data
scaler = StandardScaler()
X_train = scaler.fit_transform(
  df_train[['rain_1_3_c',  'rain_4_10_c',  'temp_1_3_c', 'temp_4_10_c', 
  'dd_1_3_c',  'dd_4_10_c',  'r20_1_3_c',  'r20_4_10_c', 'hd_1_3_c',
  'hd_4_10_c']]
  )
  
X_test = scaler.transform(df_test)

```

## 8.1. Context-free MAB

```{python}
# Model 
mab = MAB(actions, LearningPolicy.EpsilonGreedy(epsilon=0.4), seed=200)

# Train
mab.fit(
  decisions=df_train['decision_arm'], 
  rewards= df_train['reward_hetero'])

# Test
prediction = mab.predict(X_test)

expectations = mab.predict_expectations(X_test)
```

```{python}
foo = pd.Series(prediction)
foo.count()

foo1 = pd.DataFrame(expectations)
```

```{r}
table(reticulate::py$foo)
```
```{r}
res_df <- cbind(reticulate::py$foo,
reticulate::py$foo1)

colnames(res_df)[1] <- "predictied_action"
sum(
sum(res_df$SRC[res_df$predictied_action=="SRC"],na.rm = T),
sum(res_df$AC[res_df$predictied_action=="AC"],na.rm = T),
sum(res_df$SQ[res_df$predictied_action=="SQ"],na.rm = T))
```


## 8.2. Contextual MAB

```{python}
# LinUCB learning policy with alpha 1.25 and l2_lambda 1
linucb = MAB(arms=actions, 
             learning_policy=LearningPolicy.LinUCB(alpha=0.25, l2_lambda=0))

lints = MAB(actions, LearningPolicy.LinTS(alpha=0.1))

# TreeBandit learning policy w/o hyper-parameter tuning
treebandit = MAB(actions, 
                 LearningPolicy.UCB1(1), 
                 NeighborhoodPolicy.TreeBandit(tree_parameters={'min_samples_split':3,'max_depth':2,
               'min_samples_leaf':3}))

# Radius contextual policy with radius equals to 5 and ucb1 learning with alpha 1.25
radius = MAB(
 arms=actions,
 learning_policy=LearningPolicy.EpsilonGreedy(0),
 neighborhood_policy=NeighborhoodPolicy.Radius(radius=3))

```


```{python}
# Learn from previous ads shown and revenues generated
linucb.fit(
  decisions=df_train['decision_arm'], 
  rewards= df_train['reward_hetero'],
  contexts=X_train)
  
lints.fit(
  decisions=df_train['decision_arm'], 
  rewards= df_train['reward_hetero_norm'], 
  contexts=X_train)
  
# Learn from previous ads shown and revenues generated
treebandit.fit(
  decisions=df_train['decision_arm'], 
  rewards= df_train['reward_hetero_norm'], 
  contexts=X_train)


# Learn from previous ads shown and revenues generated
radius.fit(
  decisions=df_train['decision_arm'], 
  rewards= df_train['reward_hetero_norm'], 
  contexts=X_train)


# Predict the next best ad to show
prediction_linucb = linucb.predict(X_test)
prediction_lints = lints.predict(X_test)
prediction_treebandit = treebandit.predict(X_test)
prediction_radius = radius.predict(X_test)
```


```{r}
dfc %>% 
 mutate(pred_land_use=reticulate::py$prediction_treebandit) %>% 
 ggplot(color = "transparent", size = 0) +
 geom_sf(aes(fill=pred_land_use)) +
 theme_void(base_size = 36) +
 scale_fill_manual(
  "Predicted optimal\nland use type",
  values =  met.brewer("Gauguin",
                       3,
                       type = c("discrete")),
  breaks = c("AC", "SRC", "SQ"),
  labels = c("Agroforestry\n", "Short rotation\ncoppice", "Crop rotation\n")) +
 theme(legend.position = c(0.9, 0.9))
```


```{python}
foo = pd.Series(prediction)
foo.count()


foo1 = pd.DataFrame(expectations)
foo1.describe()
```


```{r}
res_df <- cbind(reticulate::py$foo,
reticulate::py$foo1)

colnames(res_df)[1] <- "predictied_action"
sum(
sum(res_df$SRC[res_df$predictied_action=="SRC"],na.rm = T),
sum(res_df$AC[res_df$predictied_action=="AC"],na.rm = T),
sum(res_df$SQ[res_df$predictied_action=="SQ"],na.rm = T))
```

```{r}
table(reticulate::py$foo)
```
```{r}
dfc %>% 
 mutate(pred_land_use=reticulate::py$foo) %>% 
 ggplot() +
 geom_sf(aes(fill=pred_land_use)) +
 theme_void()
```

### 8.2.2 Contextual tree bandit
```{python}
# Predict the next best ad to show
prediction = treebandit.predict(X_test)

# Expectation of each action based on learning from past revenues
expectations = treebandit.predict_expectations(X_test)

foo = pd.Series(prediction)
foo1 = pd.DataFrame(expectations)
```

```{r}
dfc %>% 
 mutate(pred_land_use=reticulate::py$foo) %>% 
 ggplot() +
 geom_sf(aes(fill=pred_land_use)) +
 theme_void()
```
### 8.2.3 Radius
```{python}
########################################################
# Radius Neighborhood Policy with UCB1 Learning Policy
########################################################

# Radius contextual policy with radius equals to 5 and ucb1 learning with alpha 1.25
radius = MAB(
 arms=actions,
 learning_policy=LearningPolicy.EpsilonGreedy(0),
 neighborhood_policy=NeighborhoodPolicy.Radius(radius=4))

# Learn from previous ads shown and revenues generated
radius.fit(
  decisions=df_train['decision_arm'], 
  rewards= df_train['reward_hetero_norm'], 
  contexts=X_train)

# Predict the next best ad to show
prediction = radius.predict(X_test)

# Expectation of each ad based on learning from past ad revenues
expectations = radius.predict_expectations(X_test)
```


# Hyperparameter tuning

## LinUBC
```{python, output=False, warning=False, error=False}
# Bandits to simulate
n_jobs = 4
hyper_parameter_tuning = []
for j in np.arange(0, 4, 0.25):
 for i in np.arange(0, 4, 0.5):
    hyper_parameter_tuning.append(('alpha '+str(j)+' lambda'+str(i),
                                  MAB(actions, 
                                      LearningPolicy.LinUCB(alpha=i,l2_lambda=j),
                                      n_jobs=n_jobs)))
                                      

sim_linucb = Simulator(hyper_parameter_tuning, 
                decisions=df_train['decision_arm'], 
                rewards= df_train['reward_hetero_norm'], 
                contexts=X_train,
                test_size=0.5, 
                is_ordered=False,
                batch_size=0)
                
```

```{python echo = T, results = 'hide'}
sim_linucb.run()

results = []

for mab_name, mab in sim.bandits:
      results.append(
       {
       'model': mab_name,
       'SRC': sim.bandit_to_arm_to_stats_max[mab_name]['SRC']['sum'],
       'AC':sim.bandit_to_arm_to_stats_max[mab_name]['AC']['sum'],
       'SQ':sim.bandit_to_arm_to_stats_max[mab_name]['SQ']['sum']
       }
     )

results=pd.DataFrame(results)
```


```{r}
py$results$model[which.max(rowSums(py$results[,2:4]))]
```



## Decision tree
```{python, output=False, warning=False, error=False}
# Bandits to simulate
hyper_parameter_tuning = []

for j in range(1,4):
 for i in range(1,4):
  for k in range(1,4):
   for l in np.arange(0, 2, 0.5):
    hyper_parameter_tuning.append(
     ('alpha_'+ str(l) + '_min_split_'+str(j) + '_max_depth_'+str(i) + '_min_samples_leaf_'+str(k),
     MAB(actions, 
         LearningPolicy.UCB1(l), 
         NeighborhoodPolicy.TreeBandit(
               tree_parameters={'min_samples_split':j,'max_depth':i,
               'min_samples_leaf':k}))
     )
    )


sim_tree = Simulator(hyper_parameter_tuning, 
                decisions=df_train['decision_arm'], 
                rewards= df_train['reward_hetero_norm'], 
                contexts=X_train,
                test_size=0.5, 
                is_ordered=False,
                batch_size=0)
                
```

```{python echo = T, results = 'hide'}
sim_tree.run()

results = []

for mab_name, mab in sim.bandits:
      results.append(
       {
       'model': mab_name,
       'SRC_averse': sim.bandit_to_arm_to_stats_min[mab_name]['SRC']['sum'],
       'AC_averse':sim.bandit_to_arm_to_stats_min[mab_name]['AC']['sum'],
       'SQ_averse':sim.bandit_to_arm_to_stats_min[mab_name]['SQ']['sum'],
       'SRC_neutral': sim.bandit_to_arm_to_stats_avg[mab_name]['SRC']['sum'],
       'AC_neutral':sim.bandit_to_arm_to_stats_avg[mab_name]['AC']['sum'],
       'SQ_neutral':sim.bandit_to_arm_to_stats_avg[mab_name]['SQ']['sum'],
       'SRC_seeking': sim.bandit_to_arm_to_stats_max[mab_name]['SRC']['sum'],
       'AC_seeking':sim.bandit_to_arm_to_stats_max[mab_name]['AC']['sum'],
       'SQ_seeking':sim.bandit_to_arm_to_stats_max[mab_name]['SQ']['sum']
       }
     )

results=pd.DataFrame(results)
```


```{r}
py$results$cum_reward_averse <- rowSums(py$results[,2:4], na.rm = T)
py$results$cum_reward_neutral <- rowSums(py$results[,5:7], na.rm = T)
py$results$cum_reward_seeking <- rowSums(py$results[,8:10], na.rm = T)

py$results$model[py$results$cum_reward_averse==max(py$results$cum_reward_averse)]
py$results$model[py$results$cum_reward_neutral==max(py$results$cum_reward_neutral)]
py$results$model[py$results$cum_reward_seeking==max(py$results$cum_reward_seeking)]

```

## Radius

```{python}
# Bandits to simulate
hyper_parameter_tuning = []

for radius in range(6, 10):
    hyper_parameter_tuning.append(('Radius'+str(radius),
                                  MAB(actions, LearningPolicy.UCB1(1),
                                      NeighborhoodPolicy.Radius(radius))))
                                      

sim_radius = Simulator(hyper_parameter_tuning, 
                decisions=df_train['decision_arm'], 
                rewards= df_train['reward_hetero_norm'], 
                contexts=X_train,
                test_size=0.5, 
                is_ordered=False,
                batch_size=100)
                
                
                
sim_radius.run()
```


```{python}
results = []

for mab_name, mab in sim_radius.bandits:
      results.append(
       {
       # 'model': mab_name,
       'SRC_averse': sim.bandit_to_arm_to_stats_min[mab_name]['SRC']['sum'],
       'AC_averse':sim.bandit_to_arm_to_stats_min[mab_name]['AC']['sum'],
       'SQ_averse':sim.bandit_to_arm_to_stats_min[mab_name]['SQ']['sum'],
       'SRC_neutral': sim.bandit_to_arm_to_stats_avg[mab_name]['SRC']['sum'],
       'AC_neutral':sim.bandit_to_arm_to_stats_avg[mab_name]['AC']['sum'],
       'SQ_neutral':sim.bandit_to_arm_to_stats_avg[mab_name]['SQ']['sum'],
       'SRC_seeking': sim.bandit_to_arm_to_stats_max[mab_name]['SRC']['sum'],
       'AC_seeking':sim.bandit_to_arm_to_stats_max[mab_name]['AC']['sum'],
       'SQ_seeking':sim.bandit_to_arm_to_stats_max[mab_name]['SQ']['sum']
       }
     )

results=pd.DataFrame(results)


for mab_name, mab in sim.bandits:
    print(mab_name + "\n")

    # Since the simulation is online, print the 'total' stats
    print('Worst Case Scenario:', sim.bandit_to_arm_to_stats_min[mab_name]['total'])
    print('Average Case Scenario:', sim.bandit_to_arm_to_stats_avg[mab_name]['total'])
    print('Best Case Scenario:', sim.bandit_to_arm_to_stats_max[mab_name]['total'], "\n\n")
```
```{python}
sim.bandit_to_arm_to_stats_min['radius3']
```

# Simulation compare algos
```{python}
from mabwiser.simulator import Simulator

bandits = [('1', linucb), ('2', treebandit), ('3', radius)]

sim = Simulator(bandits, 
                decisions=df_train['decision_arm'], 
                rewards= df_train['reward_hetero_norm'], 
                contexts=X_train,
                test_size=0.5, 
                is_ordered=False,
                batch_size=0,
                )
  
sim.run()
sim.plot(metric='max')
```